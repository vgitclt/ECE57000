{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPxA1gVjCF4IIVHz3nn7V8j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vgitclt/ECE57000/blob/main/JudgeLLM_Final_EXT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notebook Author: Student ECE57000\n",
        "\n",
        "# **LLM-as-a-judge** is a common technique to evaluate LLM-powered products.\n",
        "It grew in popularity for a reason: it’s a practical alternative to costly human evaluation when assessing open-ended text outputs. We can see it as an automated testing tool for LLMs effectiveness\n",
        "Judging generated texts is tricky — whether it's a “simple” summary or a chatbot conversation. Metrics like accuracy don’t work well because there are many ways to be “right” without exactly matching the example answer. And things like style or tone are subjective and hard to pin down.\n",
        "Humans can handle these nuances, but manually reviewing every response doesn’t scale. LLM-as-a-judge emerged as an alternative: you can use LLMs to evaluate the generated texts. Interestingly, the LLM is both the source of the problem and the solution!"
      ],
      "metadata": {
        "id": "J3Q-17Qy-2JW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke6_JVElR_vq"
      },
      "outputs": [],
      "source": [
        "!pip install llama-index\n",
        "!pip install llama-index-llms-huggingface\n",
        "!pip install llama-index-embeddings-huggingface\n",
        "!pip install llama-index-embeddings-huggingface-api"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. Setup LLMs from hugging face that we want to do model API inference on, as we don't have local resources and setup\n",
        "1.   LLama3\n",
        "2.   Mistral\n",
        "3.   Deepseek\n",
        "\n",
        "**Note HF_TOKEN must be set in your Colab secrets and access granted. Ensure you have GPU paid tokens for A100 as the free T4 gpus has limitations and timeouts.\n",
        "**\n",
        "\n"
      ],
      "metadata": {
        "id": "kUdganEiAvge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: Log in to Hugging Face and setup LLMs\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "stopping_ids = [\n",
        "    tokenizer.eos_token_id,\n",
        "    tokenizer.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
        "]\n",
        "\n",
        "tokenizer2 = AutoTokenizer.from_pretrained(\n",
        "    \"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "stopping_ids2 = [\n",
        "    tokenizer2.eos_token_id,\n",
        "    tokenizer2.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
        "]\n",
        "\n",
        "tokenizer3 = AutoTokenizer.from_pretrained(\n",
        "    \"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "    token=hf_token,\n",
        ")\n",
        "\n",
        "stopping_ids3 = [\n",
        "    tokenizer3.eos_token_id,\n",
        "    tokenizer3.convert_tokens_to_ids(\"<|eot_id|>\"),\n",
        "]\n",
        "\n",
        "import torch\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "BnSb1DyPTw9G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Call the huggingface LLM object to instatiate the LLMs with the keyword arguments and tokenizers"
      ],
      "metadata": {
        "id": "xt4TYLYxGQmY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_kwargs parameters are taken from https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct\n",
        "\n",
        "import torch\n",
        "from llama_index.llms.huggingface import HuggingFaceLLM\n",
        "\n",
        "# Optional quantization to 4bit\n",
        "# import torch\n",
        "# from transformers import BitsAndBytesConfig\n",
        "\n",
        "# quantization_config = BitsAndBytesConfig(\n",
        "#     load_in_4bit=True,\n",
        "#     bnb_4bit_compute_dtype=torch.float16,\n",
        "#     bnb_4bit_quant_type=\"nf4\",\n",
        "#     bnb_4bit_use_double_quant=True,\n",
        "# )\n",
        "\n",
        "#Instantiate LLama3 LLM\n",
        "llm = HuggingFaceLLM(\n",
        "    model_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    model_kwargs={\n",
        "        \"token\": hf_token,\n",
        "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
        "        # \"quantization_config\": quantization_config\n",
        "    },\n",
        "    generate_kwargs={\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.4,\n",
        "        \"top_p\": 0.9,\n",
        "    },\n",
        "    tokenizer_name=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    stopping_ids=stopping_ids,\n",
        ")\n",
        "\n",
        "#Instantiate Mistral LLM\n",
        "llm2 = HuggingFaceLLM(\n",
        "    model_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    model_kwargs={\n",
        "        \"token\": hf_token,\n",
        "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
        "        # \"quantization_config\": quantization_config\n",
        "    },\n",
        "    generate_kwargs={\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.4,\n",
        "        \"top_p\": 0.9,\n",
        "    },\n",
        "    tokenizer_name=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    stopping_ids=stopping_ids2,\n",
        ")\n",
        "\n",
        "#Instantiate Deepseek LLM\n",
        "llm3 = HuggingFaceLLM(\n",
        "    model_name=\"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "    model_kwargs={\n",
        "        \"token\": hf_token,\n",
        "        \"torch_dtype\": torch.bfloat16,  # comment this line and uncomment below to use 4bit\n",
        "        # \"quantization_config\": quantization_config\n",
        "    },\n",
        "    generate_kwargs={\n",
        "        \"do_sample\": True,\n",
        "        \"temperature\": 0.4,\n",
        "        \"top_p\": 0.9,\n",
        "    },\n",
        "    tokenizer_name=\"deepseek-ai/deepseek-llm-7b-chat\",\n",
        "    tokenizer_kwargs={\"token\": hf_token},\n",
        "    stopping_ids=stopping_ids2,\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "dqVdbhPIUGEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Import the REACT agent Llamaindex framework.\n",
        "\n",
        "LlamaIndex is an open-source data orchestration framework that simplifies building large language model (LLM) applications by providing tools for data ingestion, indexing, and retrieval, enabling context-rich AI applications through a Retrieval-Augmented Generation (RAG) pipeline. LlamaIndex is designed to make it easier to connect diverse data sources to LLMs, allowing developers to create applications that can access and leverage external knowledge. **LlamaIndex ReAct ** agent is an agent-based chat mode that uses a reasoning and acting loop to answer questions, leveraging tools and external knowledge sources to achieve more precise answers"
      ],
      "metadata": {
        "id": "NSx0bAF9G7X8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "from typing import Sequence, List\n",
        "from llama_index.core.llms import ChatMessage\n",
        "from llama_index.core.tools import BaseTool, FunctionTool\n",
        "from llama_index.core.agent import ReActAgent\n",
        "\n",
        "import nest_asyncio\n",
        "\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "id": "OAbXFJn5VaUx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Define some agent tools for Math when questions have math in them for the LLM to answer"
      ],
      "metadata": {
        "id": "-Z4gam3BIbep"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Agent tools for Math questions evaluations when judge LLM needs to evauate the Math questions.\n",
        "\n",
        "def multiply(a: int, b: int) -> int:\n",
        "    \"\"\"Multiple two integers and returns the result integer\"\"\"\n",
        "    return a * b\n",
        "\n",
        "\n",
        "def add(a: int, b: int) -> int:\n",
        "    \"\"\"Add two integers and returns the result integer\"\"\"\n",
        "    return a + b\n",
        "\n",
        "\n",
        "def subtract(a: int, b: int) -> int:\n",
        "    \"\"\"Subtract two integers and returns the result integer\"\"\"\n",
        "    return a - b\n",
        "\n",
        "\n",
        "def divide(a: int, b: int) -> int:\n",
        "    \"\"\"Divides two integers and returns the result integer\"\"\"\n",
        "    return a / b\n",
        "\n",
        "multiply_tool = FunctionTool.from_defaults(fn=multiply)\n",
        "add_tool = FunctionTool.from_defaults(fn=add)\n",
        "subtract_tool = FunctionTool.from_defaults(fn=subtract)\n",
        "divide_tool = FunctionTool.from_defaults(fn=divide)"
      ],
      "metadata": {
        "id": "pDgxowBiVtR6"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#React agent using math tools sample\n",
        "agent = ReActAgent.from_tools(\n",
        "    [multiply_tool, add_tool, subtract_tool, divide_tool],\n",
        "    llm=llm,\n",
        "    verbose=True,\n",
        ")"
      ],
      "metadata": {
        "id": "f43CdCjtVwm7"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#response = agent.chat(\"What is (121 + 2) * 5?\")\n",
        "#print(str(response))"
      ],
      "metadata": {
        "id": "cULUK_pUV3KU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#response = agent.chat(\"What is (100/5)*2-5+10 ?\")\n",
        "#print(str(response))"
      ],
      "metadata": {
        "id": "D4giFYJUV6PS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5YR0uYvZLzTX"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Download the MT-Bench data from huggingface where human judge have provided expert conclusions to LLMs' answers for questions in multiple categories."
      ],
      "metadata": {
        "id": "PSWzpyHVIr0M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages for hugging face datasets\n",
        "!pip install datasets\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datasets import load_dataset\n",
        "\n",
        "#HuggingFace data set downloaded into JSON format\n",
        "dataset = load_dataset(\"lmsys/mt_bench_human_judgments\")\n",
        "dataset[\"human\"].to_json(\"human_judgments.json\")\n",
        "dataset[\"gpt4_pair\"].to_json(\"gpt4_pair_judgments.json\")\n",
        "\n",
        "df = pd.DataFrame(dataset[\"human\"])\n",
        "print(dataset[\"human\"])\n",
        "df.head(5)\n",
        "#question = df['conversation_a']\n",
        "#print(question)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "mOCkEQQzgIpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Read data from MT-Bench and evaluate using the REACT agent tools if the LLM judge agreed with the human expert. Then the judge becomes a student LLM using Mistral and DeepSeek to and helps answer the question posed and we compare how well the student LLMs did to the winning question using text similarities. This is the main code but due to GPU unpredictability we cannot run for all 3355 rows of data, we will show case 1 row for the concept.\n",
        "\n",
        "***A100 paid GPU in Google Colab runs a single LLM inference in 30 seconds sometimes to 15 mins other times , this is due to limitations and resource demand caps.***"
      ],
      "metadata": {
        "id": "zm_p-DzFJLxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from llama_index.core import PromptTemplate\n",
        "\n",
        "react_system_header_str = \"\"\"\\\n",
        "\n",
        "You are designed to help with a task, for answering questions \\\n",
        "    to providing summaries into other types of questions acting as an LLM judge.\n",
        "\n",
        "keep the response to 250 words.\n",
        "\n",
        "## Tools\n",
        "You have access to a search tool. You are responsible for using\n",
        "that tool in any sequence you deem appropriate to complete the task at hand.\n",
        "This may require breaking the task into subtasks and using different searches\n",
        "to complete each subtask.\n",
        "\n",
        "You have access to the following tools:\n",
        "{tool}\n",
        "\n",
        "## Output Format\n",
        "To answer the question, please use the following format.\n",
        "\n",
        "```\n",
        "Thought: I need to use a tool to help me answer the question.\n",
        "Action: tool name (one of {tool_names}) if using a tool.\n",
        "Action Input: the input to the tool, in a JSON format representing the kwargs (e.g. {{\"input\": \"hello world\", \"num_beams\": 5}})\n",
        "```\n",
        "\n",
        "Please ALWAYS start with a Thought.\n",
        "\n",
        "Please use a valid JSON format for the Action Input. Do NOT do this {{'input': 'hello world', 'num_beams': 5}}.\n",
        "\n",
        "If this format is used, the user will respond in the following format:\n",
        "\n",
        "```\n",
        "Observation: tool response\n",
        "```\n",
        "\n",
        "You should keep repeating the above format until you have enough information\n",
        "to answer the question without using any more tools. At that point, you MUST respond\n",
        "in the one of the following two formats:\n",
        "\n",
        "```\n",
        "Thought: I can answer without using any more tools.\n",
        "Answer: [your answer here]\n",
        "```\n",
        "\n",
        "```\n",
        "Thought: I cannot answer the question with the provided tools.\n",
        "Answer: Sorry, I cannot answer your query.\n",
        "```\n",
        "\n",
        "## Additional Rules\n",
        "- The answer MUST contain a sequence of bullet points that explain how you arrived at the answer. This can include aspects of the previous conversation history.\n",
        "- You MUST obey the function signature of each tool. Do NOT pass in no arguments if the function expects arguments.\n",
        "\n",
        "## Current Conversation\n",
        "Below is the current conversation consisting of interleaving human and assistant messages.\n",
        "\n",
        "\"\"\"\n",
        "react_system_prompt = PromptTemplate(react_system_header_str)\n",
        "#print(agent.get_prompts())\n",
        "\n",
        "\n",
        "def compare(a: str, b: str) -> str:\n",
        "    \"\"\"compares two strings and returns the result string for the LLM\"\"\"\n",
        "    return \"which is better answer a or b? answer in 1 letter\"\n",
        "\n",
        "def answer(a: str) -> str:\n",
        "    \"\"\"compares two strings and returns the result string for the LLM\"\"\"\n",
        "    return \"You are a LLM that is asked to answer a topic that will be judged, please give you best answer in 500 words or less\"\n",
        "\n",
        "CATEGORIES = [\"Writing\", \"Roleplay\", \"Reasoning\", \"Math\", \"Coding\", \"Extraction\", \"STEM\", \"Humanities\"]\n",
        "\n",
        "def get_model_df():\n",
        "    cnt = 0\n",
        "    q2result = []\n",
        "    fin = open(\"human_judgments.json\", \"r\")\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "        obj[\"category\"] = CATEGORIES[(obj[\"question_id\"]-81)//10]\n",
        "        q2result.append(obj)\n",
        "    df = pd.DataFrame(q2result)\n",
        "    return df\n",
        "\n",
        "df = get_model_df()\n",
        "\n",
        "# loop through 3355 row for LLM inferencing is VERY challeging.\n",
        "for index, row in df.iterrows():\n",
        "    print(f\"Index: {index}, Row: {row.to_dict()}\")\n",
        "    if index == 10:\n",
        "        break\n",
        "\n",
        "num_rows = df.shape[0]\n",
        "\n",
        "print(f\"\\nNumber of rows in original human evaluated data set: {num_rows}\")\n",
        "\n",
        "# so we only use 1 row to prove the theory instead of looping though 3354 rows.\n",
        "question = df['conversation_a']\n",
        "a_qa = question.iloc[0]\n",
        "#print(a_qa.count)\n",
        "for idx, a_qa in enumerate(a_qa):\n",
        "    #print(f\"A Content {idx + 1}: {a_qa['content']}\")\n",
        "    if idx == 0:\n",
        "        #print(\"Model A:\" + a_qa['content'])\n",
        "        ResponseA_Q = a_qa['content']\n",
        "    if idx == 1:\n",
        "        #print(\"Model A:\" + a_qa['content'])\n",
        "        ResponseA = \"Answer A:\" + a_qa['content']\n",
        "\n",
        "question = df['conversation_b']\n",
        "b_qa = question.iloc[0]\n",
        "#print(b_qa.count)\n",
        "for idx, b_qa in enumerate(b_qa):\n",
        "    #print(f\"B Content {idx + 1}: {b_qa['content']}\")\n",
        "    if idx == 1:\n",
        "        #print(\"Model B:\" +b_qa['content'])\n",
        "        ResponseB = \"Answer B:\" +b_qa['content']\n",
        "\n",
        "chatIn = ResponseA + \"\\n\" + ResponseB + \"\\n\"\n",
        "chatIn2 = ResponseA_Q + \"\\n\" + \"summarize in 700 words\"\n",
        "#print(chatIn)\n",
        "compare_tool = FunctionTool.from_defaults(fn=compare)\n",
        "\n",
        "# LLM will judge the content of 2 answers and predict the best answer that we will compare to human judgements.\n",
        "#chat_mode=\"react\"\n",
        "agent = ReActAgent.from_tools([compare_tool],\n",
        "                 llm=llm,\n",
        "                 verbose=True,max_iterations=1)\n",
        "response = agent.chat(chatIn)\n",
        "agent_res = str(response)\n",
        "print(\"\\nStart Llama3 wih ReACT agent response-----------------------------------\")\n",
        "print(agent_res.lower())\n",
        "print(\"---------------------------------------------------------------------End\\n\")\n",
        "\n",
        "# now create 2 new LLMs to help answer a topic, LLMs becomes students now.\n",
        "\n",
        "res1 = llm2.complete(chatIn2)\n",
        "print(\"Start Mistral LLM response-----------------------------------\")\n",
        "print(res1)\n",
        "print(\"----------------------------------------------------------end\\n\")\n",
        "res2 = llm3.complete(chatIn2)\n",
        "print(\"Start DeepSeek LLM response-----------------------------------\")\n",
        "print(res2)\n",
        "print(\"----------------------------------------------------------end\\n\")\n",
        "winner = df['winner'].iloc[0]\n",
        "print(winner)\n",
        "qid = df['question_id'].iloc[0]\n",
        "print(qid)\n",
        "cat = df['category'].iloc[0]\n",
        "print(cat)\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Initialize an empty DataFrame\n",
        "df_judge = pd.DataFrame()\n",
        "df_student = pd.DataFrame()\n",
        "\n",
        "#response = \"B\"  # Replace with actual response logic, for testing only use\n",
        "\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# # Define the two texts to compare, for testing only use\n",
        "# text1 = \"The quick brown fox jumps over the lazy dog.\"\n",
        "# text2 = \"A fast brown fox leaps over a sleepy dog.\"\n",
        "\n",
        "# Define the two texts to compare, one from mistral, the other deepseek\n",
        "mistral_text1 = res1.text\n",
        "if winner == \"model_a\":\n",
        "    text2 = a_qa['content']\n",
        "else:\n",
        "    text2 = b_qa['content']\n",
        "\n",
        "print(text2)\n",
        "# Convert the texts into TF-IDF feature vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([mistral_text1, text2])\n",
        "\n",
        "# Compute the cosine similarity between the two texts\n",
        "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "similarity_score_model_a = f'{cosine_sim[0][0]:.2f}'\n",
        "print(similarity_score_model_a)\n",
        "# Output the similarity score\n",
        "print(f\"Cosine Similarity: {cosine_sim[0][0]:.4f}\")\n",
        "\n",
        "deepseek_text2 = res2.text\n",
        "if winner == \"model_a\":\n",
        "    text2 = a_qa['content']\n",
        "else:\n",
        "    text2 = b_qa['content']\n",
        "\n",
        "print(text2)\n",
        "# Convert the texts into TF-IDF feature vectors\n",
        "vectorizer = TfidfVectorizer()\n",
        "tfidf_matrix = vectorizer.fit_transform([deepseek_text2, text2])\n",
        "\n",
        "# Compute the cosine similarity between the two texts\n",
        "cosine_sim = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])\n",
        "similarity_score_model_b = f'{cosine_sim[0][0]:.2f}'\n",
        "print(similarity_score_model_b)\n",
        "# Output the similarity score\n",
        "print(f\"Cosine Similarity: {cosine_sim[0][0]:.4f}\")\n",
        "\n",
        "if similarity_score_model_a > similarity_score_model_b:\n",
        "    student_winner = \"model_a\"\n",
        "else:\n",
        "    student_winner = \"model_b\"\n",
        "\n",
        "# Simulate data generation in a loop for judge data capture\n",
        "for i in range(1):\n",
        "    new_row = {\"LLMWinner\": agent_res.lower(), \"HumanWinner\": winner, \"QuestionID\": qid, \"Category\": cat}  # Define a new row\n",
        "    df_judge = pd.concat([df_judge, pd.DataFrame([new_row])], ignore_index=True)  # Append the new row\n",
        "\n",
        "# Simulate data generation in a loop for student data capture\n",
        "for i in range(1):\n",
        "    new_row = {\"Model_A\": \"Mistral\", \"Model_B\": \"DeepSeek\", \"Winner\": student_winner, \"SimilarityToWinner_model_a\": similarity_score_model_a, \"SimilarityToWinner_model_b\": similarity_score_model_b, \"Category\": cat}  # Define a new row\n",
        "    df_student = pd.concat([df_student, pd.DataFrame([new_row])], ignore_index=True)  # Append the new row\n",
        "\n",
        "# Print the resulting DataFrame\n",
        "print(df_judge)\n",
        "print(df_student)\n",
        "\n"
      ],
      "metadata": {
        "id": "jzspWm0xGrxL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Original implementation code to Analyse agreements between gpt4 and humans and human to human."
      ],
      "metadata": {
        "id": "7so9140Envqe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_judge_name(judge):\n",
        "    if isinstance(judge, list) and judge[0] == \"gpt-4\" and judge[1].startswith(\"pair\"):\n",
        "        return \"gpt4-pair\"\n",
        "    if judge.startswith(\"expert\"):\n",
        "        return \"human\"\n",
        "    if judge.startswith(\"author\"):\n",
        "        return \"author\"\n",
        "    return judge\n",
        "\n",
        "\n",
        "def revert(vote):\n",
        "    if vote == \"model_a\":\n",
        "        return \"model_b\"\n",
        "    elif vote == \"model_b\":\n",
        "        return \"model_a\"\n",
        "    return vote\n",
        "\n",
        "\n",
        "def get_mt_bench_votes_data(raw_votes):\n",
        "    data = [{}, {}]\n",
        "\n",
        "    for judge_votes in raw_votes:\n",
        "        for vote in judge_votes:\n",
        "            turn = vote[\"turn\"] - 1\n",
        "            if vote[\"model_a\"] < vote[\"model_b\"]:\n",
        "                key = (vote[\"question_id\"], vote[\"model_a\"], vote[\"model_b\"])\n",
        "                winner = vote[\"winner\"]\n",
        "            else:\n",
        "                key = (vote[\"question_id\"], vote[\"model_b\"], vote[\"model_a\"])\n",
        "                winner = revert(vote[\"winner\"])\n",
        "            judge = get_judge_name(vote[\"judge\"])\n",
        "            if key not in data[turn]:\n",
        "                data[turn][key] = {}\n",
        "            if judge not in data[turn][key]:\n",
        "                data[turn][key][judge] = []\n",
        "            data[turn][key][judge].append(winner)\n",
        "\n",
        "    return data\n",
        "\n",
        "\n",
        "def convertvote(vote):\n",
        "    if \"tie\" in vote:\n",
        "        return \"tie\"\n",
        "    return vote\n",
        "\n",
        "\n",
        "def equalvote(vote1, vote2):\n",
        "    if \"tie\" in vote1 and \"tie\" in vote2:\n",
        "        return True\n",
        "    return vote1 == vote2\n",
        "\n",
        "\n",
        "# data: Dict[qid -> List[vote]]\n",
        "def get_mt_bench_agreement(data, judge1, judge2, ban):\n",
        "    if judge1.startswith(\"gpt4\") and judge2 == \"human\":\n",
        "        stats = [0, 0]\n",
        "        for votes in data.values():\n",
        "            if judge1 not in votes or judge2 not in votes: continue\n",
        "            assert len(votes[judge1]) == 1\n",
        "            if convertvote(votes[judge1][0]) in ban: continue\n",
        "            for v in votes[judge2]:\n",
        "                if convertvote(v) in ban: continue\n",
        "                stats[1] += 1\n",
        "                stats[0] += equalvote(votes[judge1][0], v)\n",
        "        return stats[0], stats[1]\n",
        "    elif judge1 == \"human\" and judge2 == \"human\":\n",
        "        stats = [0, 0]\n",
        "        for votes in data.values():\n",
        "            if \"human\" not in votes: continue\n",
        "            for i in range(len(votes[\"human\"]) - 1):\n",
        "                for j in range(i + 1, len(votes[\"human\"])):\n",
        "                    if convertvote(votes[\"human\"][i]) in ban or convertvote(votes[\"human\"][j]) in ban:\n",
        "                        continue\n",
        "                    stats[1] += 1\n",
        "                    stats[0] += equalvote(votes[\"human\"][i], votes[\"human\"][j])\n",
        "        return stats[0], stats[1]\n",
        "    else:\n",
        "        raise Exception(\"Unsupported judges.\")\n",
        "\n",
        "\n",
        "def run_mt_bench_agreement(judges, votefiles):\n",
        "    # votes[i]: List of votes\n",
        "    votes = []\n",
        "    for filename in votefiles:\n",
        "        data = []\n",
        "        for line in open(filename, \"r\"):\n",
        "            data.append(json.loads(line))\n",
        "        votes.append(data)\n",
        "\n",
        "    data = get_mt_bench_votes_data(votes)\n",
        "\n",
        "    agree, total = get_mt_bench_agreement(data[0], judges[0], judges[1], ban=[])\n",
        "    print(f\"turn 1 with tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\")\n",
        "    agree, total = get_mt_bench_agreement(data[0], judges[0], judges[1], ban=[\"tie\"])\n",
        "    print(f\"turn 1 without tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\")\n",
        "    agree, total = get_mt_bench_agreement(data[1], judges[0], judges[1], ban=[])\n",
        "    print(f\"turn 2 with tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\")\n",
        "    agree, total = get_mt_bench_agreement(data[1], judges[0], judges[1], ban=[\"tie\"])\n",
        "    print(f\"turn 2 without tie. #total: {total}, #agree: {agree}, ratio: {agree/total:.2f}\")"
      ],
      "metadata": {
        "id": "fe38jgJEIBig"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Checking for agreements, win/lose or tie GPT4 and Human"
      ],
      "metadata": {
        "id": "SiOP-Lj9ole6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute agrement between GPT-4 and humans\n",
        "run_mt_bench_agreement([\"gpt4_pair\", \"human\"], [\"gpt4_pair_judgments.json\", \"human_judgments.json\"])"
      ],
      "metadata": {
        "id": "rey2aKQ5gO24",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3cabddb-416a-41a5-b1c5-70c4dd7bd6ca"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "turn 1 with tie. #total: 1343, #agree: 886, ratio: 0.66\n",
            "turn 1 without tie. #total: 859, #agree: 727, ratio: 0.85\n",
            "turn 2 with tie. #total: 1325, #agree: 871, ratio: 0.66\n",
            "turn 2 without tie. #total: 864, #agree: 731, ratio: 0.85\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9. Checking for agreements, win/lose or tie Human to Human"
      ],
      "metadata": {
        "id": "dzf8T1_xo3C8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute agrement between humans and humans\n",
        "run_mt_bench_agreement([\"human\", \"human\"], [\"human_judgments.json\"])"
      ],
      "metadata": {
        "id": "foqM_mcsgbvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13ade249-2c32-4117-9b46-03260b843530"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "turn 1 with tie. #total: 721, #agree: 454, ratio: 0.63\n",
            "turn 1 without tie. #total: 479, #agree: 388, ratio: 0.81\n",
            "turn 2 with tie. #total: 707, #agree: 471, ratio: 0.67\n",
            "turn 2 without tie. #total: 474, #agree: 388, ratio: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10. Plot for Scoring The judgement data but due to GPU constraints, we were able to showcase 1 row, intention to showcase cell 11 from original implementation."
      ],
      "metadata": {
        "id": "CBLe4epZpJcs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "import plotly.express as px\n",
        "#df = px.data.wind()\n",
        "print(df_judge)\n",
        "fig = px.scatter_polar(df_judge, r=\"QuestionID\", theta=\"Category\")\n",
        "fig.show()\n"
      ],
      "metadata": {
        "id": "qOiI7hofWFGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14dc91b-fa22-4b99-c75d-f87970c3588e",
        "id": "osWaWZMZjdfu"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-12 16:44:35--  https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/76c55033c6b2b1cc3f62513458f84748a23352495fd42b1062a7401de5ff9bd9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gpt-4_single.jsonl%3B+filename%3D%22gpt-4_single.jsonl%22%3B&Expires=1744479876&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDQ3OTg3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0Lzc2YzU1MDMzYzZiMmIxY2MzZjYyNTEzNDU4Zjg0NzQ4YTIzMzUyNDk1ZmQ0MmIxMDYyYTc0MDFkZTVmZjliZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=kEYkmp1ZnohFcoKuMC-JL12KgT95GpuZ%7EJnoL6UqMabVZy%7EzqhroJ1%7E3wWfdQcop-gAi197b0XjoAmfBG7l1nu1xT-AdwYFaLP-DrY4KdpsYc9e3RXHFAQ2%7EHH-MrFBL-IjTsssH%7EoaihUt6C-3tHxmvfi9mHvw6sVKRLNX-rjuH1qB6QyijOFiNZkpUyJMghTQ94UThvQwHf-%7EdXRBO9m-VCJvgvuRmTArtOUqjTOst1COGOTHkMqaSr2f7bgFtprJVu8FuMx-cIe9lglRrFB%7EwjFOB8Kdg%7EMS9rX1b0etmCQGcTWGPbSuedyx0ic6ehzYmS3gnvzYXfVW8wI-gWw__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-04-12 16:44:36--  https://cdn-lfs.hf.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/76c55033c6b2b1cc3f62513458f84748a23352495fd42b1062a7401de5ff9bd9?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gpt-4_single.jsonl%3B+filename%3D%22gpt-4_single.jsonl%22%3B&Expires=1744479876&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDQ3OTg3Nn19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0Lzc2YzU1MDMzYzZiMmIxY2MzZjYyNTEzNDU4Zjg0NzQ4YTIzMzUyNDk1ZmQ0MmIxMDYyYTc0MDFkZTVmZjliZDk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=kEYkmp1ZnohFcoKuMC-JL12KgT95GpuZ%7EJnoL6UqMabVZy%7EzqhroJ1%7E3wWfdQcop-gAi197b0XjoAmfBG7l1nu1xT-AdwYFaLP-DrY4KdpsYc9e3RXHFAQ2%7EHH-MrFBL-IjTsssH%7EoaihUt6C-3tHxmvfi9mHvw6sVKRLNX-rjuH1qB6QyijOFiNZkpUyJMghTQ94UThvQwHf-%7EdXRBO9m-VCJvgvuRmTArtOUqjTOst1COGOTHkMqaSr2f7bgFtprJVu8FuMx-cIe9lglRrFB%7EwjFOB8Kdg%7EMS9rX1b0etmCQGcTWGPbSuedyx0ic6ehzYmS3gnvzYXfVW8wI-gWw__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.165.102.115, 3.165.102.127, 3.165.102.128, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.165.102.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 20113128 (19M) [text/plain]\n",
            "Saving to: ‘gpt-4_single.jsonl’\n",
            "\n",
            "gpt-4_single.jsonl  100%[===================>]  19.18M  22.5MB/s    in 0.9s    \n",
            "\n",
            "2025-04-12 16:44:37 (22.5 MB/s) - ‘gpt-4_single.jsonl’ saved [20113128/20113128]\n",
            "\n",
            "--2025-04-12 16:44:37--  https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl\n",
            "Resolving huggingface.co (huggingface.co)... 13.35.202.97, 13.35.202.121, 13.35.202.34, ...\n",
            "Connecting to huggingface.co (huggingface.co)|13.35.202.97|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.hf.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/d662c0b7d1d297f0494fcb4cc09fe8f054fa22d75deb4754a483a921984bc585?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gpt-4_pair.jsonl%3B+filename%3D%22gpt-4_pair.jsonl%22%3B&Expires=1744479877&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDQ3OTg3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0L2Q2NjJjMGI3ZDFkMjk3ZjA0OTRmY2I0Y2MwOWZlOGYwNTRmYTIyZDc1ZGViNDc1NGE0ODNhOTIxOTg0YmM1ODU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=RFG9GtM9%7Emb-1mJI2pJGJ6ncDe1epEZ8dfufwMerkbljLI%7Ef6okWk0gRt09RKnOEfGC03iHgYlXFnsMBnJwFj1AZWu6Fk99BFtp8h8G9Nf4Nz6xPpp7rQeUX1LX7SoVK8Z-9F1ikViFNl6iSXJsvb0806u-Vt74qkEPG6X9wq81ceM2H8k6YqjM00N75AVb%7EE6-YuB59dP1tLq4Eb8i2ArUI4b28dmAiwrgLoM7ILO0-Gr%7Eg20HqP3wi9rG3taMpv5uQqHXmJ7H8WfAbm4z5dPpk0bbc6RwVD8XkzBx3KmxiCHOfqLECNIHGygHyxXIdkv1OgULNKf9LjLvgLWUZrQ__&Key-Pair-Id=K3RPWS32NSSJCE [following]\n",
            "--2025-04-12 16:44:37--  https://cdn-lfs.hf.co/repos/12/2b/122bd8e9eccbb3acc98acf73e0ecef3c96f24dcdb5f6639074ed304eb19f9cd4/d662c0b7d1d297f0494fcb4cc09fe8f054fa22d75deb4754a483a921984bc585?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27gpt-4_pair.jsonl%3B+filename%3D%22gpt-4_pair.jsonl%22%3B&Expires=1744479877&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc0NDQ3OTg3N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5oZi5jby9yZXBvcy8xMi8yYi8xMjJiZDhlOWVjY2JiM2FjYzk4YWNmNzNlMGVjZWYzYzk2ZjI0ZGNkYjVmNjYzOTA3NGVkMzA0ZWIxOWY5Y2Q0L2Q2NjJjMGI3ZDFkMjk3ZjA0OTRmY2I0Y2MwOWZlOGYwNTRmYTIyZDc1ZGViNDc1NGE0ODNhOTIxOTg0YmM1ODU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=RFG9GtM9%7Emb-1mJI2pJGJ6ncDe1epEZ8dfufwMerkbljLI%7Ef6okWk0gRt09RKnOEfGC03iHgYlXFnsMBnJwFj1AZWu6Fk99BFtp8h8G9Nf4Nz6xPpp7rQeUX1LX7SoVK8Z-9F1ikViFNl6iSXJsvb0806u-Vt74qkEPG6X9wq81ceM2H8k6YqjM00N75AVb%7EE6-YuB59dP1tLq4Eb8i2ArUI4b28dmAiwrgLoM7ILO0-Gr%7Eg20HqP3wi9rG3taMpv5uQqHXmJ7H8WfAbm4z5dPpk0bbc6RwVD8XkzBx3KmxiCHOfqLECNIHGygHyxXIdkv1OgULNKf9LjLvgLWUZrQ__&Key-Pair-Id=K3RPWS32NSSJCE\n",
            "Resolving cdn-lfs.hf.co (cdn-lfs.hf.co)... 3.165.102.115, 3.165.102.127, 3.165.102.128, ...\n",
            "Connecting to cdn-lfs.hf.co (cdn-lfs.hf.co)|3.165.102.115|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 48043462 (46M) [binary/octet-stream]\n",
            "Saving to: ‘gpt-4_pair.jsonl’\n",
            "\n",
            "gpt-4_pair.jsonl    100%[===================>]  45.82M  22.4MB/s    in 2.0s    \n",
            "\n",
            "2025-04-12 16:44:40 (22.4 MB/s) - ‘gpt-4_pair.jsonl’ saved [48043462/48043462]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_single.jsonl\n",
        "!wget https://huggingface.co/spaces/lmsys/mt-bench/resolve/main/data/mt_bench/model_judgment/gpt-4_pair.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11. Original Implementation code below to show a plot of how well the chosen models performed in different categories by GPT-4 as the judge and GPT-4 evaluates itself too."
      ],
      "metadata": {
        "id": "tIFRWHUmjvFb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "\n",
        "\n",
        "CATEGORIES = [\"Writing\", \"Roleplay\", \"Reasoning\", \"Math\", \"Coding\", \"Extraction\", \"STEM\", \"Humanities\"]\n",
        "\n",
        "\n",
        "def get_model_df():\n",
        "    cnt = 0\n",
        "    q2result = []\n",
        "    fin = open(\"gpt-4_single.jsonl\", \"r\")\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "        obj[\"category\"] = CATEGORIES[(obj[\"question_id\"]-81)//10]\n",
        "        q2result.append(obj)\n",
        "        #print(obj[\"score\"])\n",
        "    df = pd.DataFrame(q2result)\n",
        "    return df\n",
        "\n",
        "def toggle(res_str):\n",
        "    if res_str == \"win\":\n",
        "        return \"loss\"\n",
        "    elif res_str == \"loss\":\n",
        "        return \"win\"\n",
        "    return \"tie\"\n",
        "\n",
        "def get_model_df_pair():\n",
        "    fin = open(\"gpt-4_pair.jsonl\", \"r\")\n",
        "    cnt = 0\n",
        "    q2result = []\n",
        "    for line in fin:\n",
        "        obj = json.loads(line)\n",
        "\n",
        "        result = {}\n",
        "        result[\"qid\"] = str(obj[\"question_id\"])\n",
        "        result[\"turn\"] = str(obj[\"turn\"])\n",
        "        if obj[\"g1_winner\"] == \"model_1\" and obj[\"g2_winner\"] == \"model_1\":\n",
        "            result[\"result\"] = \"win\"\n",
        "        elif obj[\"g1_winner\"] == \"model_2\" and obj[\"g2_winner\"] == \"model_2\":\n",
        "            result[\"result\"] = \"loss\"\n",
        "        else:\n",
        "            result[\"result\"] = \"tie\"\n",
        "        result[\"category\"] = CATEGORIES[(obj[\"question_id\"]-81)//10]\n",
        "        result[\"model\"] = obj[\"model_1\"]\n",
        "        q2result.append(result)\n",
        "\n",
        "    df = pd.DataFrame(q2result)\n",
        "\n",
        "    return df\n",
        "\n",
        "df = get_model_df()\n",
        "df_pair = get_model_df_pair()\n",
        "all_models = df[\"model\"].unique()\n",
        "print(all_models)\n",
        "scores_all = []\n",
        "for model in all_models:\n",
        "    for cat in CATEGORIES:\n",
        "        # filter category/model, and score format error (<1% case)\n",
        "        res = df[(df[\"category\"]==cat) & (df[\"model\"]==model) & (df[\"score\"] >= 0)]\n",
        "        score = res[\"score\"].mean()\n",
        "        scores_all.append({\"model\": model, \"category\": cat, \"score\": score})\n",
        "\n",
        "target_models = [\"Llama-2-7b-chat\", \"Llama-2-13b-chat\", \"Llama-2-70b-chat\", \"gpt-3.5-turbo\", \"claude-v1\", \"gpt-4\"]\n",
        "\n",
        "scores_target = [scores_all[i] for i in range(len(scores_all)) if scores_all[i][\"model\"] in target_models]\n",
        "\n",
        "# sort by target_models\n",
        "scores_target = sorted(scores_target, key=lambda x: target_models.index(x[\"model\"]), reverse=True)\n",
        "\n",
        "df_score = pd.DataFrame(scores_target)\n",
        "df_score = df_score[df_score[\"model\"].isin(target_models)]\n",
        "\n",
        "rename_map = {\"llama-13b\": \"LLaMA-13B\",\n",
        "              \"alpaca-13b\": \"Alpaca-13B\",\n",
        "              \"vicuna-33b-v1.3\": \"Vicuna-33B\",\n",
        "              \"vicuna-13b-v1.3\": \"Vicuna-13B\",\n",
        "              \"gpt-3.5-turbo\": \"GPT-3.5-turbo\",\n",
        "              \"claude-v1\": \"Claude-v1\",\n",
        "              \"gpt-4\": \"GPT-4\"}\n",
        "\n",
        "for k, v in rename_map.items():\n",
        "    df_score.replace(k, v, inplace=True)\n",
        "\n",
        "fig = px.line_polar(df_score, r = 'score', theta = 'category', line_close = True, category_orders = {\"category\": CATEGORIES},\n",
        "                    color = 'model', markers=True, color_discrete_sequence=px.colors.qualitative.Pastel)\n",
        "\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "pECSAMNUiXoN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!pip freeze > /content/drive/MyDrive/requirements1.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPrnSdi60Eba",
        "outputId": "d366f47d-893b-46bd-a6e8-43fca9e2a4d7"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    }
  ]
}